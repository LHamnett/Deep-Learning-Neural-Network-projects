{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network with Tensorflow for MNIST Classification\n",
    "\n",
    "## Author/Data-Scientist: Leon Hamnett\n",
    "\n",
    "### [LinkedIn](https://www.linkedin.com/in/leon-hamnett/)\n",
    "\n",
    "#### Contents:\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Preprocessing](#prepro)\n",
    "3. [Building the model](#model)\n",
    "4. [Training the model](#train)\n",
    "5. [Testing and Conclusion](#test)\n",
    "\n",
    "### Introduction: <a name=\"introduction\"></a>\n",
    "\n",
    "In this notebook, we will use Tensorflow to create a deep neural network to apply a machine learning algorithm to classify/recognise handwritten digits.\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition. This is a well known dataset within the machine learning community. The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image).  The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes. \n",
    "\n",
    "We will build a neural network with 3 hidden layers to achieve this goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data <a name=\"prepro\"></a>\n",
    "\n",
    "In this section we will load our data from the MNIST dataset into an appropiate format and then preprocess it so it can be easily fed into the machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "#load dataset and info about the MNIST data\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "#extract training and test datasets\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "# by default, TF has training and testing datasets, but no validation sets thus we will split it on our own\n",
    "\n",
    "# we start by defining the number of validation samples as a % of the train samples\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "# we will cast this number to an integer, as a float may cause an error along the way\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "#We will also store the number of test samples in a dedicated variable (instead of using the mnist_info one)\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "# change to integer\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "\n",
    "# normally, we would like to scale our data in some way to make the result more numerically stable\n",
    "# in this case we will simply prefer to have inputs between 0 and 1\n",
    "\n",
    "# define a scale function that takes an image as well as the label (target) for that image\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)\n",
    "    # We  can divide each element by 255 so that all elements will be between 0 and 1 \n",
    "    image /= 255.\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# we have already decided that we will get the validation data from mnist_train, so assign a variable  for this and apply the scale function\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "# finally, we scale and batch the test data\n",
    "# we scale it so it has the same magnitude as the train and validation\n",
    "# there is no need to shuffle it, because we won't be training on the test data\n",
    "# there would be a single batch, equal to the size of the test data\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "\n",
    "# We will also shuffle the data\n",
    "\n",
    "#set buffer size for larger datasets\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# shuffle the training and validation data\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# once we have scaled and shuffled the data, we can proceed to separating the train and validation data\n",
    "# our validation data would be equal to 10% of the training set, which we've already calculated\n",
    "# we use the .take() method to take that many samples\n",
    "# finally, we create a batch with a batch size equal to the total number of validation samples\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "# set the batch size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# we can also take advantage of the occasion to batch the train data\n",
    "# this would be very helpful when we train, as we would be able to iterate over the different batches\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "# batch the test data\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "\n",
    "# takes next batch (it is the only batch)\n",
    "# because as_supervized=True, we've got a 2-tuple structure\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model <a name=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline the model\n",
    "\n",
    "In this section we will design and code the deep neural net that we will use to employ the machine learning model. We set an input size equal to the number of pixels in each image as well as an output equal to the 10 classifications that are possible (0-9) from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input size = 784 as there is 28 x 28 pixels in each image\n",
    "input_size = 784\n",
    "# output size = 10, as 10 possible numbers, we will use one-hot encoding fo each output/target\n",
    "output_size = 10\n",
    "# Use same hidden layer size for both hidden layers for simplicity\n",
    "hidden_layer_size = 100\n",
    "    \n",
    "# define how the model will look like\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    # the first layer (the input layer)\n",
    "    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3\n",
    "    # need to flatten the image\n",
    "    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) \n",
    "    # or (28x28x1,) = (784,) vector\n",
    "    # this allows us to actually create a feed forward neural network\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n",
    "    \n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 3rd hidden layer\n",
    "    \n",
    "    # the final layer is no different, we just make sure to activate it with softmax to get probabilities\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the optimizer we'd like to use, \n",
    "# the loss function, \n",
    "# and the metrics we are interested in obtaining at each iteration\n",
    "# we choose adam optimizer as this is most cutting edge in machine learning \n",
    "# we choose the appropiate loss mechanism for out categorical data\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training <a name=\"train\"></a>\n",
    "\n",
    "In this section we will train the machine learning model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "540/540 - 20s - loss: 0.3176 - accuracy: 0.9064 - val_loss: 0.1530 - val_accuracy: 0.9560\n",
      "Epoch 2/10\n",
      "540/540 - 29s - loss: 0.1223 - accuracy: 0.9632 - val_loss: 0.1133 - val_accuracy: 0.9670\n",
      "Epoch 3/10\n",
      "540/540 - 28s - loss: 0.0876 - accuracy: 0.9731 - val_loss: 0.0841 - val_accuracy: 0.9752\n",
      "Epoch 4/10\n",
      "540/540 - 33s - loss: 0.0679 - accuracy: 0.9785 - val_loss: 0.0728 - val_accuracy: 0.9772\n",
      "Epoch 5/10\n",
      "540/540 - 38s - loss: 0.0555 - accuracy: 0.9826 - val_loss: 0.0526 - val_accuracy: 0.9840\n",
      "Epoch 6/10\n",
      "540/540 - 26s - loss: 0.0440 - accuracy: 0.9858 - val_loss: 0.0501 - val_accuracy: 0.9845\n",
      "Epoch 7/10\n",
      "540/540 - 26s - loss: 0.0358 - accuracy: 0.9891 - val_loss: 0.0452 - val_accuracy: 0.9858\n",
      "Epoch 8/10\n",
      "540/540 - 27s - loss: 0.0324 - accuracy: 0.9894 - val_loss: 0.0395 - val_accuracy: 0.9878\n",
      "Epoch 9/10\n",
      "540/540 - 24s - loss: 0.0306 - accuracy: 0.9900 - val_loss: 0.0322 - val_accuracy: 0.9887\n",
      "Epoch 10/10\n",
      "540/540 - 24s - loss: 0.0237 - accuracy: 0.9920 - val_loss: 0.0353 - val_accuracy: 0.9892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f490c328100>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the maximum number of epochs\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# we fit the model, specifying the\n",
    "# training data\n",
    "# the total number of epochs\n",
    "# and the validation data we just created ourselves in the format: (inputs,targets)\n",
    "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model <a name=\"test\"></a>\n",
    "\n",
    "Now after training the model on the training data and validating on the validation data, we test the final prediction power of our model by running it on the test dataset that the algorithm has never seen before.\n",
    "\n",
    "It is very important to realize that fiddling with the hyperparameters overfits the validation dataset. \n",
    "\n",
    "The test is the absolute final instance. We should not test before we are completely done with adjusting your model.\n",
    "\n",
    "If we adjust the model after testing, we can start to see some overfitting on the test dataset, which will defeat its purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0930 - accuracy: 0.9765\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.09. Test accuracy: 97.65%\n"
     ]
    }
   ],
   "source": [
    "# We can apply some nice formatting if we want to\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "We can see from the testing accuracy that the model can correctly predict roughly 97% of handwritten digits when presented in the format of a centered 28 x 28 grayscale image. Depending on the reason the deep neural net model is being deployed this could be an acceptable level of accuracy. However if even further accuracy is required, more hidden layers could be added, the effect of using different activation functions could be investigated and also if more data could be obtained to train the model, this would also increase the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2]",
   "language": "python",
   "name": "conda-env-py3-TF2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
